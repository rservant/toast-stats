# Data Pipeline Workflow
# Runs daily: scrape â†’ transform â†’ compute-analytics â†’ upload to GCS
#
# Prerequisites:
#   1. Run scripts/setup-wif.sh (one-time)
#   2. Add GitHub secrets: GCP_PROJECT_ID, GCP_WORKLOAD_IDENTITY_PROVIDER, GCP_SERVICE_ACCOUNT
#
# The collector uses Playwright (chromium) to scrape Toastmasters dashboards.
# Data is processed locally and uploaded to GCS for the backend to consume.

name: Data Pipeline

on:
  schedule:
    # Run daily at 13:00 UTC (8 AM EST / 9 AM EDT)
    - cron: '0 13 * * *'
  workflow_dispatch:
    inputs:
      date:
        description: 'Target date (YYYY-MM-DD), defaults to today'
        required: false
        type: string
      districts:
        description: 'Comma-separated district IDs (e.g. 57,58,59), defaults to all'
        required: false
        type: string
      force:
        description: 'Force re-scrape even if cache exists'
        required: false
        type: boolean
        default: false

# Prevent overlapping pipeline runs
concurrency:
  group: data-pipeline
  cancel-in-progress: false

env:
  NODE_VERSION: '20'

permissions:
  contents: read
  id-token: write # Required for Workload Identity Federation

jobs:
  data-pipeline:
    name: Scrape, Transform, Analyze & Upload
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Node.js
        uses: actions/setup-node@v6
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Build packages
        run: |
          npm run build --workspace=@toastmasters/shared-contracts
          npm run build --workspace=@toastmasters/analytics-core
          npm run build --workspace=@toastmasters/collector-cli

      - name: Install Playwright browsers
        run: npx playwright install chromium --with-deps

      - name: Authenticate to GCP
        uses: google-github-actions/auth@v3
        with:
          workload_identity_provider: ${{ secrets.GCP_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ secrets.GCP_SERVICE_ACCOUNT }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v3

      # â”€â”€ Step 1: Discover districts + sync raw-csv cache from GCS â”€â”€â”€â”€â”€â”€â”€â”€
      # Districts are auto-discovered by downloading the Toastmasters
      # districtsummary CSV for the target date and parsing the DISTRICT column.
      # Manual workflow_dispatch 'districts' input overrides auto-discovery.
      # Previously used config/districts.json from GCS (now deprecated, kept in
      # place but no longer read). Companion to backend change #139.
      - name: Discover districts and sync raw-csv from GCS
        id: config
        env:
          GCS_BUCKET: 'toast-stats-data'
        run: |
          # Toastmasters dashboard is always one day behind â€” use yesterday
          TARGET_DATE="${{ inputs.date || '' }}"
          if [ -z "${TARGET_DATE}" ]; then
            TARGET_DATE=$(date -u -d '1 day ago' +%Y-%m-%d)
          fi
          echo "Target date: ${TARGET_DATE}"

          # If districts were manually specified via workflow_dispatch, use them directly
          if [ -n "${{ inputs.districts }}" ]; then
            DISTRICTS="${{ inputs.districts }}"
            echo "Using manual district override: ${DISTRICTS}"
          else
            # Auto-discover districts from the Toastmasters districtsummary CSV
            PROGRAM_YEAR=$(node -e "
              const d = new Date('${TARGET_DATE}');
              const y = d.getFullYear(); const m = d.getMonth();
              console.log(m >= 6 ? y+'-'+(y+1) : (y-1)+'-'+y);
            ")
            DATE_FORMATTED=$(date -d "${TARGET_DATE}" +'%-m/%-d/%Y')
            DISTRICTS_CSV_URL="https://dashboards.toastmasters.org/${PROGRAM_YEAR}/export.aspx?type=CSV&report=districtsummary~${DATE_FORMATTED}~~${PROGRAM_YEAR}"

            echo "Discovering districts from: ${DISTRICTS_CSV_URL}"
            DISTRICTS=$(curl -sf --retry 3 --max-time 30 "${DISTRICTS_CSV_URL}" | node -e "
              const data = require('fs').readFileSync('/dev/stdin','utf8');
              const lines = data.trim().split('\n');
              const headers = lines[0].split(',').map(h => h.replace(/\"/g,'').trim());
              const col = headers.indexOf('DISTRICT');
              const ids = [...new Set(lines.slice(1).map(l => l.split(',')[col].replace(/\"/g,'').trim()).filter(Boolean))];
              ids.sort((a,b) => { const na=parseInt(a,10),nb=parseInt(b,10); return isNaN(na)||isNaN(nb)?a.localeCompare(b):na-nb; });
              process.stdout.write(ids.join(','));
            ")

            if [ -z "${DISTRICTS}" ]; then
              echo "::error::Failed to discover districts from Toastmasters CSV. The dashboard may be down or the CSV format changed."
              exit 1
            fi
            echo "Discovered $(echo "${DISTRICTS}" | tr ',' '\n' | wc -l | tr -d ' ') districts"
          fi

          echo "districts=${DISTRICTS}" >> "$GITHUB_OUTPUT"

          # Sync raw-csv cache for target date
          mkdir -p "./cache/raw-csv/${TARGET_DATE}"
          gsutil -m rsync -r \
            "gs://${GCS_BUCKET}/raw-csv/${TARGET_DATE}/" \
            "./cache/raw-csv/${TARGET_DATE}/" \
            2>&1 || echo "No existing raw-csv for ${TARGET_DATE} in GCS (first run for this date)"

          echo "## â¬ District Discovery & Raw CSV Sync" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Districts**: ${DISTRICTS}" >> "$GITHUB_STEP_SUMMARY"
          FILE_COUNT=$(find "./cache/raw-csv/${TARGET_DATE}" -name '*.csv' 2>/dev/null | wc -l | tr -d ' ')
          echo "- **Date**: ${TARGET_DATE}" >> "$GITHUB_STEP_SUMMARY"
          echo "- **CSV files restored**: ${FILE_COUNT}" >> "$GITHUB_STEP_SUMMARY"


      # â”€â”€ Step 2: Scrape + Transform â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Scrape & Transform
        id: scrape
        env:
          TOASTMASTERS_DASHBOARD_URL: 'https://dashboards.toastmasters.org'
        run: |
          ARGS="--transform --verbose"

          # Use manual date if provided, otherwise default to yesterday
          # (Toastmasters dashboard is always one day behind)
          if [ -n "${{ inputs.date }}" ]; then
            ARGS="$ARGS --date ${{ inputs.date }}"
          else
            ARGS="$ARGS --date $(date -u -d '1 day ago' +%Y-%m-%d)"
          fi

          # District list comes from Step 1: auto-discovered from Toastmasters CSV
          # or overridden by the manual workflow_dispatch input
          DISTRICT_LIST="${{ steps.config.outputs.districts }}"
          ARGS="$ARGS --districts ${DISTRICT_LIST}"


          # Force flag
          if [ "${{ inputs.force }}" = "true" ]; then
            ARGS="$ARGS --force"
          fi

          echo "Running: npx collector-cli scrape $ARGS"
          npx collector-cli scrape $ARGS | tee /tmp/scrape-output.json

          # Extract date from output for subsequent steps
          DATE=$(jq -r '.date' /tmp/scrape-output.json)
          echo "date=${DATE}" >> "$GITHUB_OUTPUT"

          echo "## ðŸ“¥ Scrape Results" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Date**: ${DATE}" >> "$GITHUB_STEP_SUMMARY"
          SUCCEEDED=$(jq '.districts.succeeded' /tmp/scrape-output.json)
          FAILED=$(jq '.districts.failed' /tmp/scrape-output.json)
          echo "- **Districts succeeded**: ${SUCCEEDED}" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Districts failed**: ${FAILED}" >> "$GITHUB_STEP_SUMMARY"

      # â”€â”€ Step 3: Sync time-series from GCS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      # The TimeSeriesIndexWriter appends today's data point to the
      # existing program-year index. Without prior data, it would
      # create a fresh index with only 1 data point each run.
      # This syncs ~344 KB of index files â€” not the full 7+ GB cache.
      - name: Sync time-series from GCS
        env:
          GCS_BUCKET: 'toast-stats-data'
        run: |
          echo "Syncing time-series index from GCS..."
          mkdir -p ./cache/time-series
          gsutil -m rsync -r \
            "gs://${GCS_BUCKET}/time-series/" \
            "./cache/time-series/" \
            2>&1 || echo "No existing time-series data in GCS (first run)"

          echo "## â¬ Time-Series Sync" >> "$GITHUB_STEP_SUMMARY"
          FILE_COUNT=$(find ./cache/time-series -name '*.json' 2>/dev/null | wc -l | tr -d ' ')
          echo "- **Index files restored**: ${FILE_COUNT}" >> "$GITHUB_STEP_SUMMARY"

      # â”€â”€ Step 3b: Sync previous year snapshots for YoY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      # The AnalyticsComputer needs the previous program year's
      # snapshot to compute year-over-year comparisons. Without it,
      # the YoY file is written with dataAvailable: false.
      # Syncs all district_*.json files in parallel using gsutil -m cp (#142)
      - name: Sync previous year snapshots from GCS
        env:
          GCS_BUCKET: 'toast-stats-data'
        run: |
          DATE="${{ steps.scrape.outputs.date }}"

          # Calculate previous year date (GNU date on Ubuntu runners)
          PREV_YEAR=$(date -d "${DATE} - 1 year" +%Y-%m-%d)

          echo "Syncing previous year snapshots for YoY comparison (${PREV_YEAR})..."

          # Parallel batch copy â€” replaces sequential per-district loop (#142)
          mkdir -p "./cache/snapshots/${PREV_YEAR}"
          gsutil -m cp \
            "gs://${GCS_BUCKET}/snapshots/${PREV_YEAR}/district_*.json" \
            "./cache/snapshots/${PREV_YEAR}/" \
            2>/dev/null || true
          SYNCED=$(find "./cache/snapshots/${PREV_YEAR}" -name 'district_*.json' 2>/dev/null | wc -l | tr -d ' ')

          echo "## â¬ Previous Year Snapshot Sync" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Previous year date**: ${PREV_YEAR}" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Snapshots synced**: ${SYNCED}" >> "$GITHUB_STEP_SUMMARY"

      # â”€â”€ Step 3c: Sync club-trends store from GCS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      # The ClubTrendsStore accumulates per-club membership and DCP trend
      # data across pipeline runs (#144). Without this sync, each run would
      # start fresh with only today's data point.
      - name: Sync club-trends store from GCS
        env:
          GCS_BUCKET: 'toast-stats-data'
        run: |
          echo "Syncing club-trends store from GCS..."
          mkdir -p ./cache/club-trends
          gsutil -m rsync -r \
            "gs://${GCS_BUCKET}/club-trends/" \
            "./cache/club-trends/" \
            2>&1 || echo "No existing club-trends in GCS (first run)"

          echo "## â¬ Club-Trends Sync" >> "$GITHUB_STEP_SUMMARY"
          CT_COUNT=$(find ./cache/club-trends -name '*.json' 2>/dev/null | wc -l | tr -d ' ')
          echo "- **Store files restored**: ${CT_COUNT}" >> "$GITHUB_STEP_SUMMARY"

      # â”€â”€ Step 4: Compute Analytics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Compute Analytics
        run: |
          DATE="${{ steps.scrape.outputs.date }}"
          DISTRICT_LIST="${{ steps.config.outputs.districts }}"
          ARGS="--date ${DATE} --districts ${DISTRICT_LIST} --verbose"

          echo "Running: npx collector-cli compute-analytics $ARGS"
          npx collector-cli compute-analytics $ARGS | tee /tmp/analytics-output.json

          echo "## ðŸ“Š Analytics Results" >> "$GITHUB_STEP_SUMMARY"
          SUCCEEDED=$(jq '.districts.succeeded' /tmp/analytics-output.json)
          echo "- **Districts computed**: ${SUCCEEDED}" >> "$GITHUB_STEP_SUMMARY"

      # â”€â”€ Step 5: Sync cache to GCS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      # Single rsync replaces collector-cli upload + separate syncs.
      # gsutil rsync compares against remote state â€” truly incremental
      # with no local manifest needed (which was useless on ephemeral runners).
      - name: Sync cache to GCS
        env:
          GCS_BUCKET: 'toast-stats-data'
        run: |
          DATE="${{ steps.scrape.outputs.date }}"
          echo "Syncing cache to GCS..."

          # Sync raw-csv (only today's date)
          gsutil -m rsync -r \
            "./cache/raw-csv/${DATE}/" \
            "gs://${GCS_BUCKET}/raw-csv/${DATE}/"

          # Sync snapshots + analytics (only today's date)
          gsutil -m rsync -r \
            "./cache/snapshots/${DATE}/" \
            "gs://${GCS_BUCKET}/snapshots/${DATE}/"

          # Sync time-series (all â€” small, ~344 KB)
          gsutil -m rsync -r \
            "./cache/time-series/" \
            "gs://${GCS_BUCKET}/time-series/"

          # Sync club-trends store (all program years â€” small, ~50 KB/district)
          gsutil -m rsync -r \
            "./cache/club-trends/" \
            "gs://${GCS_BUCKET}/club-trends/"

          echo "## â˜ï¸ GCS Sync Results" >> "$GITHUB_STEP_SUMMARY"
          CSV_COUNT=$(find "./cache/raw-csv/${DATE}" -name '*.csv' 2>/dev/null | wc -l | tr -d ' ')
          SNAPSHOT_COUNT=$(find "./cache/snapshots/${DATE}" -name '*.json' 2>/dev/null | wc -l | tr -d ' ')
          TS_COUNT=$(find ./cache/time-series -name '*.json' 2>/dev/null | wc -l | tr -d ' ')
          echo "- **Date**: ${DATE}" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Raw CSVs synced**: ${CSV_COUNT}" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Snapshot/analytics files synced**: ${SNAPSHOT_COUNT}" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Time-series index files synced**: ${TS_COUNT}" >> "$GITHUB_STEP_SUMMARY"

      # â”€â”€ Step 6: Update district-snapshot-index (#138) â”€â”€â”€â”€â”€â”€
      # The backend reads per-district cached dates from this
      # pre-computed index. Without updating it, the district
      # detail page date selector goes stale.
      - name: Update district-snapshot-index
        env:
          GCS_BUCKET: 'toast-stats-data'
        run: |
          DATE="${{ steps.scrape.outputs.date }}"
          DISTRICT_LIST="${{ steps.config.outputs.districts }}"
          INDEX_PATH="gs://${GCS_BUCKET}/config/district-snapshot-index.json"

          echo "Updating district-snapshot-index for ${DATE}..."

          # Download existing index (or start empty)
          gsutil cp "${INDEX_PATH}" /tmp/district-snapshot-index.json 2>/dev/null || \
            echo '{"generatedAt":"","districts":{}}' > /tmp/district-snapshot-index.json

          # Update the index with today's date for each district
          node -e "
            const fs = require('fs');
            const index = JSON.parse(fs.readFileSync('/tmp/district-snapshot-index.json', 'utf-8'));
            const date = '${DATE}';
            const districts = '${DISTRICT_LIST}'.split(',').map(d => d.trim());

            for (const districtId of districts) {
              const existing = index.districts[districtId] || [];
              const dateSet = new Set(existing);
              dateSet.add(date);
              index.districts[districtId] = [...dateSet].sort();
            }
            index.generatedAt = new Date().toISOString();

            fs.writeFileSync('/tmp/district-snapshot-index.json', JSON.stringify(index, null, 2));
            console.log('Updated index for ' + districts.length + ' districts with date ' + date);
          "

          # Upload updated index
          gsutil cp /tmp/district-snapshot-index.json "${INDEX_PATH}"

          echo "## ðŸ“‹ District Snapshot Index" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Updated for date**: ${DATE}" >> "$GITHUB_STEP_SUMMARY"
          DISTRICT_COUNT=$(echo "${DISTRICT_LIST}" | tr ',' '\n' | wc -l | tr -d ' ')
          echo "- **Districts updated**: ${DISTRICT_COUNT}" >> "$GITHUB_STEP_SUMMARY"

      # â”€â”€ Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Pipeline summary
        if: always()
        run: |
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "---" >> "$GITHUB_STEP_SUMMARY"
          SCRAPE_DATE="${{ steps.scrape.outputs.date }}"
          TIMESTAMP=$(date -u +%H:%M' UTC')
          LINE="ðŸ Pipeline completed for"
          LINE="$LINE \`${SCRAPE_DATE}\`"
          LINE="$LINE at ${TIMESTAMP}"
          echo "$LINE" >> "$GITHUB_STEP_SUMMARY"
